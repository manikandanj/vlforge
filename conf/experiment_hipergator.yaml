defaults:
  - _self_

hydra:
  job:
    chdir: false
  run:
    dir: "/blue/arthur.porto-biocosmos/mjeyarajan3.gatech/butterfly_project/logs/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"
    
experiment:
  name: "embed_finetuned_bioclip_best_img2img_20250707"
  seed: 42
  device: "cuda"


# Logging configuration - let Hydra handle directory structure
logging:
  outputs_dir: null  # null = use Hydra's automatic directory structure
  log_filename: null  # null = use "main.log", specify filename for custom name
  auto_filename: false  # true = auto-generate filename with timestamp

# Mode can be "embed" or "eval"
# embed: Generate and save embeddings to HDF5 file
# eval: Run evaluation (see evaluation.use_precomputed_embeddings for H5 vs model-based)
mode: "embed"  # Change to "eval" to run evaluation

# Embedding configuration
embeddings:
  # Storage configuration
  storage_dir: "/blue/arthur.porto-biocosmos/mjeyarajan3.gatech/butterfly_project/storage"  # Directory for H5 files
  
  # Filename configuration
  auto_filename: true  # Auto-generate descriptive filenames
  custom_filename: null  # Use specific filename (overrides auto_filename if set)
  
  # For embed mode: max samples to process (null for all)
  max_samples: null  # Process full dataset (230,689 samples)

# Evaluation configuration (only used in eval mode)
# Two evaluation approaches available:
# 1. H5-based (use_precomputed_embeddings: true) - Fast, uses pre-saved embeddings
# 2. Model-based (use_precomputed_embeddings: false) - Slower, processes images in real-time
evaluation:
  # Evaluation data source configuration
  use_precomputed_embeddings: true  # true = use H5 file, false = use model + dataloader
  
  # H5-based evaluation settings (only used when use_precomputed_embeddings: true)
  # Specific embedding file to load (if null, will use most recent in storage_dir)
  embedding_file: null  # e.g., "storage/2025-07-07_14-30-52_BioCLIP_butterfly_embeddings.h5"
  
  # Query/database split configuration (for H5-based evaluation)
  split:
    query_ratio: 0.01  # 1% for queries, 99% for database
    num_queries: 10    # Number of queries to evaluate (for demo/testing)

# Comment out base BioCLIP model
# model:
#   _target_: "models.bioclip.BioCLIPModel"
#   device: "cuda"
#   model_name: "hf-hub:imageomics/bioclip"

# Fine-tuned BioCLIP model configuration
model:
  _target_: "models.finetuned_bioclip.FineTunedBioCLIPModel"
  device: "cuda"
  checkpoint_path: "/blue/arthur.porto-biocosmos/mjeyarajan3.gatech/butterfly_project/checkpoint_files/best_img2img_20250707.pt"
  base_model_name: "hf-hub:imageomics/bioclip"

# Comment other model and uncomment this one to use OpenAI CLIP
# model:
#   _target_: "models.openai_clip.OpenAICLIPModel"
#   device: "cuda"
#   model_name: "openai/clip-vit-base-patch32"


# data:
#   _target_: "dataloaders.vlmbio_loader.VLMBioDataLoader"
#   base_dir: "data/VLM4Bio/datasets"
#   group: "Fish"
#   metadata_file: "metadata_10k.csv"
#   batch_size: 128

data:
  _target_: "dataloaders.butterfly_dataset_loader.ButterflyDatasetDataLoader"
  base_dir: "/blue/arthur.porto-biocosmos/data/datasets"
  metadata_dir: "/blue/arthur.porto-biocosmos/data/datasets/nymphalidae_whole_specimen-v250613/metadata"
  group: "nymphalidae_whole_specimen-v250613"
  metadata_file: "data_meta-nymphalidae_whole_specimen-v250613.csv"
  batch_size: 1024

# Evaluation metrics - only used in eval mode
# Updated to work with pre-computed embeddings and image-to-image similarity
# Query limiting: n_samples controls how many queries to evaluate
# - n_samples: 500 → limit to 500 queries
# - n_samples: null → use ALL queries from evaluation.split
evaluators:
  # Binary relevance metrics (species-level matches only)
  - _target_: "evaluation.precision_at_k_evaluator.PrecisionAtKEvaluator"
    n_samples: 500              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    show_progress: true
    
  - _target_: "evaluation.recall_at_k_evaluator.RecallAtKEvaluator"
    n_samples: 500              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    show_progress: true
    
  - _target_: "evaluation.map_evaluator.mAPEvaluator"
    n_samples: 500              # Set to null to use all queries from split
    show_progress: true
    
  # Hierarchical relevance metrics (multi-level taxonomy with exponential gain)
  - _target_: "evaluation.ndcg_evaluator.NDCGEvaluator"
    n_samples: 500              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    primary_k: 50
    show_progress: true
