defaults:
  - _self_

hydra:
  job:
    chdir: false 
    
experiment:
  name: "Base OpenAI CLIP"
  seed: 42
  device: "cuda"

# Logging configuration
logging:
  outputs_dir: null  # null = use Hydra's output directory
  log_filename: null  # null = use "main.log", specify filename for custom name
  auto_filename: false  # true = auto-generate filename with timestamp

# Mode can be "embed", "eval", or "report"
mode: "report"  

# Embedding configuration
embeddings:
  # Storage configuration
  storage_dir: "storage"  # Directory for H5 files
  
  # Filename configuration
  auto_filename: true  # Auto-generate descriptive filenames
  custom_filename: null  # Use specific filename (overrides auto_filename if set)
  
  # For embed mode: max samples to process (null for all)
  max_samples: null

# Evaluation configuration (only used in eval mode)
# Two evaluation approaches available:
# 1. H5-based (use_precomputed_embeddings: true) - Fast, uses pre-saved embeddings
# 2. Model-based (use_precomputed_embeddings: false) - Slower, processes images in real-time
evaluation:
  # Evaluation data source configuration
  use_precomputed_embeddings: true  # true = use H5 file, false = use model + dataloader
  
  # H5-based evaluation settings (only used when use_precomputed_embeddings: true)
  # Specific embedding file to load
  embedding_file: "C:/Mani/learn/Courses/BioCosmos/Butterfly_Project/Artifacts/v1/2025-07-20_02-57-02_OpenAICLIP_clip-vit-base-patch32_butterfly_embeddings.h5"  # e.g., "storage/2025-07-07_14-30-52_BioCLIP_butterfly_embeddings.h5"
  
  # Query/database split configuration (for H5-based evaluation)
  split:
    query_ratio: 0.01  # 1% for queries, 99% for database (affects total query pool size)
    num_queries: 100    # Number of queries to evaluate (for demo/testing). Set to null to use all queries from split

# Base BioCLIP model
# model:
#   _target_: "models.bioclip.BioCLIPModel"
#   device: "cuda"
#   model_name: "hf-hub:imageomics/bioclip"

# Fine-tuned BioCLIP model configuration
# model:
#   _target_: "models.finetuned_bioclip.FineTunedBioCLIPModel"
#   device: "cuda"
#   checkpoint_path: "C:/Mani/learn/Courses/BioCosmos/Butterfly_Project/Artifacts/v1/best_img2img__v1_20250707.pt"
#   base_model_name: "hf-hub:imageomics/bioclip"

# OpenAI CLIP model 
# model:
#   _target_: "models.openai_clip.OpenAICLIPModel"
#   device: "cuda"
#   model_name: "openai/clip-vit-base-patch32"

# Base BioClip 2 model  
model:
  _target_: "models.bioclip.BioCLIPModel"
  device: "cuda"
  model_name: "hf-hub:imageomics/bioclip-2"

# data:
#   _target_: "dataloaders.vlmbio_loader.VLMBioDataLoader"
#   base_dir: "data/VLM4Bio/datasets"
#   group: "Fish"
#   metadata_file: "metadata_10k.csv"
#   batch_size: 128

data:
  _target_: "dataloaders.butterfly_dataset_loader.ButterflyDatasetDataLoader"
  base_dir: "C:\\Mani\\learn\\Courses\\BioCosmos\\Butterfly_Project\\Data"
  metadata_dir: "C:\\Mani\\learn\\Courses\\BioCosmos\\Butterfly_Project\\Data\\nymphalidae_whole_specimen-v240606\\metadata"
  # base_dir: "/blue/arthur.porto-biocosmos/data/datasets"
  # metadata_dir: "/blue/arthur.porto-biocosmos/data/datasets/nymphalidae_whole_specimen-v240606/metadata"
  group: "nymphalidae_whole_specimen-v240606"
  metadata_file: "data_meta-nymphalidae_whole_specimen-v240606_subset.csv"
  batch_size: 128

# Evaluation metrics - only used in eval mode
# Updated to work with pre-computed embeddings and image-to-image similarity
# Query limiting: n_samples controls how many queries to evaluate
# - n_samples: 500 → limit to 500 queries
# - n_samples: null → use ALL queries from evaluation.split
evaluators:
  # Binary relevance metrics (species-level matches only)
  - _target_: "evaluation.precision_at_k_evaluator.PrecisionAtKEvaluator"
    n_samples: null              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    show_progress: true
    use_gpu: true                # Enable GPU acceleration with PyTorch
    
  - _target_: "evaluation.recall_at_k_evaluator.RecallAtKEvaluator"
    n_samples: null              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    show_progress: true
    use_gpu: true                # Enable GPU acceleration with PyTorch
    
  - _target_: "evaluation.map_evaluator.mAPEvaluator"
    n_samples: null              # Set to null to use all queries from split
    show_progress: true
    use_gpu: true                # Enable GPU acceleration with PyTorch
    
  # Hierarchical relevance metrics (multi-level taxonomy with exponential gain)
  - _target_: "evaluation.ndcg_evaluator.NDCGEvaluator"
    n_samples: null              # Set to null to use all queries from split
    k_values: [1, 5, 50]
    primary_k: 50
    show_progress: true
    use_gpu: true                # Enable GPU acceleration with PyTorch

# Report configuration (only used in report mode)
# Output will be saved in the current run's Hydra output directory
report:
  input_dirs:
    - "outputs/2025-07-22/08-28-13"  
    - "outputs/2025-07-22/07-58-51"  
    - "outputs/2025-07-22/07-57-19"  


  title: "Model Performance Comparison"
  formats: ["png", "pdf"]  
  metrics: []
